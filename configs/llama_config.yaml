# Llama-specific inference settings (Llama-3.2-3B-Instruct)
model:
  repo_id: meta-llama/Llama-3.2-3B-Instruct
  local_subdir: llama3/Llama-3.2-3B-Instruct
  # If you convert to GGUF, set gguf_name to the file in the same folder
  gguf_name: "Llama-3.2-3B-Instruct.Q4_K_M.gguf"                # e.g. Llama-3.2-3B-Instruct.Q4_K_M.gguf

inference:
  backend: llama-cpp-python     # options: llama-cpp-python | transformers | custom
  use_gpu: true
  n_gpu_layers: 36              # tune down if OOM (try 0..40)
  n_ctx: 4096                   # keep at 4k for RTX3050; set to 8192 only if tested
  dtype: float16                # float16 or float32; ignored for gguf/q4
  trust_remote_code: false

quantization:
  prefer_quantized: true        # allow bench to look for gguf / q4 files
  preferred_format: Q4_K_M      # Q4_K_M, Q5_K_M, Q8_0 or empty
  quantize_tool: llama.cpp      # informative only; script can call external tool

serve:
  host: 127.0.0.1
  port: 8000
  max_batch_tokens: 1024
  max_concurrency: 2

notes: |
  - If gguf is provided, set gguf_name to the filename.
  - If using raw HF weights, set gguf_name to "" and rely on local conversion.
  - Tune n_gpu_layers to avoid OOM; the bench script will try safe defaults.
